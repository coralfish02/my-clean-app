import os
import streamlit as st
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter # æ”¹å–„ç‰ˆã®Splitterã‚’ä½¿ç”¨
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- åˆæœŸè¨­å®šï¼ˆã‚¢ãƒ—ãƒªã®åˆå›èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰ ---
@st.cache_resource
def setup_rag_chain():
    # 1. APIã‚­ãƒ¼ã®è¨­å®š (Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ©Ÿèƒ½ã‹ã‚‰èª­ã¿è¾¼ã‚€)
    try:
        os.environ["OPENAI_API_VERSION"] = st.secrets["AZURE_OPENAI"]["API_VERSION"]
        os.environ["AZURE_OPENAI_ENDPOINT"] = st.secrets["AZURE_OPENAI"]["ENDPOINT"]
        os.environ["AZURE_OPENAI_API_KEY"] = st.secrets["AZURE_OPENAI"]["API_KEY"]
        os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = st.secrets["AZURE_OPENAI"]["CHAT_DEPLOYMENT_NAME"]
        os.environ["AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"] = st.secrets["AZURE_OPENAI"]["EMBEDDING_DEPLOYMENT_NAME"]
    except KeyError:
        st.error("Streamlitã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã«Azure OpenAIã®èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰²
    loader = TextLoader("./0830LLM.txt", encoding='utf8')
    documents = loader.load()
    
    # Colabã®è­¦å‘Šã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€ã‚ˆã‚Šé«˜æ€§èƒ½ãªåˆ†å‰²æ–¹æ³•ã«å¤‰æ›´
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
    )
    docs = text_splitter.split_documents(documents)

    # 3. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨Vector Storeã®æº–å‚™ï¼ˆæ°¸ç¶šåŒ–å¯¾å¿œï¼‰
    embeddings = AzureOpenAIEmbeddings(model="text-embedding-3-small")
    
    # æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    persist_directory = "./chroma_db"
    
    # æ—¢ã«DBãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°ä½œæˆã—ã¦ä¿å­˜
    if os.path.exists(persist_directory):
        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    else:
        vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
        
    retriever = vectorstore.as_retriever()
    llm = AzureChatOpenAI(azure_deployment="gpt-4.1")

    # 4. RAGãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    template = """
    ã‚ãªãŸã¯è¦ªåˆ‡ãªç‰‡ä»˜ã‘ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
    ä»¥ä¸‹ã®ã€Œéƒ¨å±‹ã®çŠ¶æ…‹ã€ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

    # éƒ¨å±‹ã®çŠ¶æ…‹
    {context}

    # è³ªå•
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever.invoke, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# --- Streamlit UIéƒ¨åˆ† ---

# RAGãƒã‚§ãƒ¼ãƒ³ã‚’æº–å‚™
chain = setup_rag_chain()

st.title("ãŠç‰‡ä»˜ã‘ç›¸è«‡AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ ğŸ¤–")
st.caption("AIãŒã‚ãªãŸã®ãŠå®¶ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€ç‰‡ä»˜ã‘ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¾ã™ã€‚")

# ä¼šè©±å±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ç®¡ç†
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘å–ã‚‹ãƒãƒ£ãƒƒãƒˆå…¥åŠ›æ¬„
if user_input := st.chat_input("ä»Šæ—¥ã¯ã©ã“ã‚’ç‰‡ä»˜ã‘ãŸã„ã§ã™ã‹ï¼Ÿ"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦è¡¨ç¤º
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # AIã®å¿œç­”ã‚’ç”Ÿæˆ
    with st.chat_message("assistant"):
        with st.spinner("AIãŒè€ƒãˆã¦ã„ã¾ã™..."):
            response = chain.invoke(user_input)
            st.markdown(response)
    
    # AIã®å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "assistant", "content": response})